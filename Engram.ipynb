{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42eb9b8a-896d-46ae-bb94-15a0fc1d67e9",
   "metadata": {},
   "source": [
    "# Engram\n",
    "\n",
    "git: [dhcode-cpp/Engram-pytorch](https://github.com/dhcode-cpp/Engram-pytorch)\n",
    "\n",
    "Blog: [【手撕Engram】DeepSeek 的 Conditional Memory 能取代 Attention 吗？](https://zhuanlan.zhihu.com/p/1994713080131772751)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a8b01d7-9228-4047-87f9-acf5d478f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from dataclasses import dataclass, field\n",
    "import math\n",
    "\n",
    "## third-class\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c037b30-9709-46a1-8df7-3a8e53d8e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EngramModelConfig:\n",
    "    dim: int = 512\n",
    "    head_dim: int = 64\n",
    "    max_n_gram: int = 3\n",
    "    max_memory_vocab_size: int = 1007\n",
    "    head_hash: int = 8\n",
    "    n_hc: int = 4\n",
    "    num_layer: int = 4\n",
    "    vocab_size: int = 100\n",
    "    kernel_size: int = 4 # 序列卷积核\n",
    "\n",
    "config = EngramModelConfig()\n",
    "\n",
    "bsz = 2\n",
    "seq_len = 100\n",
    "\n",
    "x = torch.randint(config.vocab_size, \n",
    "                  size=(bsz, seq_len))\n",
    "\n",
    "H = torch.randn(bsz, seq_len, config.dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62693888-da61-4caf-8459-f65b8a0ac54b",
   "metadata": {},
   "source": [
    "## hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38dbb003-fe62-45c2-9bf9-c28448a2b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadsHash:\n",
    "    def __init__(self, max_memory_vocab_size, layer_id):\n",
    "        \n",
    "        self.mods = torch.tensor([12582917, 25165843, 50331653, 100663319, 201326611, 402653189, 805306457, 1610612741]) # 素数\n",
    "        self.mods *= (layer_id+1) # 每层哈希结果不同\n",
    "        self.max_memory_vocab_size = max_memory_vocab_size\n",
    "        self.layer_id = layer_id\n",
    "\n",
    "    def hash(self, x, mod, n_gram):\n",
    "        x_ = x.clone()\n",
    "        for i in range(1, n_gram):\n",
    "            x_[:, i:] *= x[:, :-i]\n",
    "        hash_id = x_ % mod \n",
    "        hash_id = hash_id % self.max_memory_vocab_size\n",
    "        return hash_id\n",
    "\n",
    "    def multi_head_hash(self, x, mods, n_gram):\n",
    "        hash_ids = []\n",
    "        for mod in mods:\n",
    "            hash_id = self.hash(x, mod, n_gram)\n",
    "            hash_ids.append(hash_id)\n",
    "        hash_ids = torch.stack(hash_ids, dim=-1) # bsz, seq_len, hash_head\n",
    "        return hash_ids\n",
    "\n",
    "    def get_all_hash_ids(self, x, max_n_gram):\n",
    "        ngram_hash_ids = []\n",
    "        for N in range(1, max_n_gram):\n",
    "            hash_ids = self.multi_head_hash(x, self.mods, N)\n",
    "            ngram_hash_ids.append(hash_ids)\n",
    "        return ngram_hash_ids # [ [bsz, seq_len, hash_head], [bsz, seq_len, hash_head] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c5d113-734c-45af-b979-606263bea39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100])\n",
      "torch.Size([2, 100, 8])\n",
      "torch.Size([2, 100, 8])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mhash = MultiHeadsHash(config.max_memory_vocab_size, layer_id = 3)\n",
    "hash_ids = mhash.get_all_hash_ids(x, config.max_n_gram)\n",
    "\n",
    "print(x.shape)\n",
    "print(hash_ids[0].shape) # 2-gram, 8-hash-head\n",
    "print(hash_ids[1].shape) # 3-gram, 8-hash-head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf19616-0dd7-4b77-814c-22677345644c",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8849c12-7095-4b75-8a0a-8da078d31aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalMemory(nn.Module):\n",
    "    def __init__(self,  \n",
    "                 config\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_dim = config.head_dim\n",
    "        max_memory_vocab_size=config.max_memory_vocab_size\n",
    "        self.head_hash = config.head_hash\n",
    "\n",
    "        # self.memory_embds = [ nn.Embedding(max_memory_vocab_size, head_dim) for i in range(head_hash) ]\n",
    "        self.memory_embds = nn.Embedding(max_memory_vocab_size * self.head_hash, self.head_dim)\n",
    "        self.offset = torch.arange(self.head_hash) * max_memory_vocab_size \n",
    "        self.offset = self.offset[None, None, 1]\n",
    "\n",
    "    def forward(self, x, ngram_hash_ids):\n",
    "        bsz, seq_len = x.shape\n",
    "        n = len(ngram_hash_ids)\n",
    "        \n",
    "        x += self.offset\n",
    "        ngram_memory = []\n",
    "        for hash_ids in ngram_hash_ids:\n",
    "            memory = self.memory_embds(hash_ids)\n",
    "            ngram_memory.append(memory)\n",
    "        h_memory = torch.cat(ngram_memory, dim = -1)\n",
    "        h_memory = h_memory.reshape(bsz, seq_len, n*self.head_hash*self.head_dim) # 提前 cat\n",
    "        return h_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e78d7203-244c-4422-9ae4-72e3e4cee6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 1024])\n"
     ]
    }
   ],
   "source": [
    "memory = ConditionalMemory(config)\n",
    "h_memory = memory(x, hash_ids)\n",
    "print(h_memory.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4911ff3e-beb6-4ca1-91d9-d89060201496",
   "metadata": {},
   "source": [
    "## Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "980d9774-da5b-4207-9d50-333e6204ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortConv1D(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 n_hc,\n",
    "                 kernel_size,):\n",
    "        super().__init__()\n",
    "        \n",
    "        dilation=1\n",
    "        self.total_dim = dim * n_hc # cat n_hc dim\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=self.total_dim,\n",
    "            out_channels=self.total_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            groups=self.total_dim,\n",
    "            bias=False,\n",
    "            padding=(kernel_size - 1) * dilation, # 3\n",
    "            dilation=dilation,\n",
    "        )\n",
    "\n",
    "        self.norms = nn.ModuleList([nn.RMSNorm( dim ) for _ in range(n_hc)])\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, n_hc, D = x.shape \n",
    "\n",
    "        x_norm = []\n",
    "        for hc_idx, norm in enumerate(self.norms):\n",
    "            x_norm.append(norm(x[:,:, hc_idx]))\n",
    "\n",
    "        x_norm = torch.stack(x_norm, dim=2) # B, L, n_hc, D \n",
    "\n",
    "        x_norm = x_norm.reshape(B, L, n_hc*D) # B, L, (n_hc, D) -> B, L, (C)\n",
    "        x_norm = x_norm.transpose(1, 2) # B, C, L\n",
    "        y = self.conv(x_norm)\n",
    "        y = y[..., :L]\n",
    "\n",
    "        y = self.act_fn(y) # swiglu\n",
    "        y = y.transpose(1,2).reshape(B, L, n_hc, D)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6de6dfe5-0793-45de-9052-dbb48719c3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 4, 1024])\n",
      "torch.Size([2, 10, 4, 1024])\n"
     ]
    }
   ],
   "source": [
    "conv = ShortConv1D(\n",
    "    dim=config.dim * (config.max_n_gram-1), \n",
    "    kernel_size=config.kernel_size,\n",
    "    n_hc=config.n_hc,\n",
    ")\n",
    "\n",
    "\n",
    "H_hc = torch.randn(2, 10, config.n_hc, config.dim*(config.max_n_gram-1))\n",
    "H_conv = conv(H_hc)\n",
    "\n",
    "print(H_hc.shape)\n",
    "print(H_conv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37074ec7-2ef3-42b6-86f4-672cbb7b6129",
   "metadata": {},
   "source": [
    "## Engram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e1b8b91-4b22-4e58-886d-b1b89789c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Engram(nn.Module):\n",
    "    def __init__(self, \n",
    "                 config,\n",
    "                 layer_id=1,\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_hc = config.n_hc\n",
    "        D = config.dim\n",
    "        \n",
    "        memory_dim = config.head_dim * config.head_hash * (config.max_n_gram-1)\n",
    "        \n",
    "        self.Wks = nn.ModuleList([ nn.Linear(memory_dim, D) for i in range(self.n_hc) ])\n",
    "        self.Wv = nn.Linear(memory_dim, D)\n",
    "        self.norm1 = [ nn.RMSNorm(D) for i in range(self.n_hc) ]\n",
    "        self.norm2 = [ nn.RMSNorm(D) for i in range(self.n_hc) ]\n",
    "\n",
    "        self.max_n_gram = config.max_n_gram\n",
    "\n",
    "        self.memory = ConditionalMemory(config)\n",
    "        \n",
    "        self.hash = MultiHeadsHash(max_memory_vocab_size=config.max_memory_vocab_size, layer_id=layer_id)\n",
    "        \n",
    "        self.conv = ShortConv1D(config.dim, \n",
    "                                config.n_hc, \n",
    "                                config.kernel_size)\n",
    "\n",
    "    def forward(self, h, x):\n",
    "        \"\"\"\n",
    "            h: bsz, seq_len, n_hc, dim\n",
    "                hidden states\n",
    "               \n",
    "            x: bsz, seq_len\n",
    "                input ids\n",
    "        \"\"\"\n",
    "        \n",
    "        _,_, _, D = h.shape\n",
    "        \n",
    "        ngram_hash_id = self.hash.get_all_hash_ids(x, self.max_n_gram)\n",
    "        h_memory = self.memory(x, ngram_hash_id)\n",
    "\n",
    "        gates = [] # Hype Connection beta\n",
    "        for hc_idx in range(self.n_hc):\n",
    "            # proj\n",
    "            q = self.norm1[hc_idx](h[:,:, hc_idx, :])\n",
    "            k = self.norm2[hc_idx](self.Wks[hc_idx](h_memory))\n",
    "            # score\n",
    "            gate = q * k / math.sqrt(D) # bsz, seq_len, 1\n",
    "            gate = torch.sigmoid(gate)\n",
    "            # gate = gate.unsqueeze(dim=1)\n",
    "            gates.append(gate)\n",
    "\n",
    "        # value\n",
    "        gates = torch.stack(gates, dim = 2) # bsz, seq_len, n_hc, 1\n",
    "        v = self.Wv(h_memory).unsqueeze(2) # bsz, seq_len, n_hc, dim\n",
    "        v_ = gates * v # bsz, seq_len, n_hc, dim\n",
    "\n",
    "        # Conv1D \n",
    "        out = self.conv(v_) + v_\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3cea1f3-188a-46b3-bd9e-bb9034a768df",
   "metadata": {},
   "outputs": [],
   "source": [
    "engram = Engram(config)\n",
    "\n",
    "bsz = 1\n",
    "seq_len = 10\n",
    "H = torch.randn(bsz, seq_len, config.n_hc, config.dim)\n",
    "x = torch.randint(config.vocab_size, (bsz, seq_len))\n",
    "y = engram(H, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b1b1a2-3dd8-42ff-943d-f81f04af6582",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b28efa27-dfc7-4d5c-af17-8192d68b6837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 with_engram,\n",
    "                 layer_id,\n",
    "                 config,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        D = config.dim\n",
    "        self.with_engram = with_engram\n",
    "        self.ffn = nn.Linear(D, D)\n",
    "        self.attn = nn.Linear(D, D)\n",
    "        if with_engram:\n",
    "            self.engram = Engram(config, layer_id=layer_id)\n",
    "        \n",
    "    def forward(self, H, x):\n",
    "        if self.with_engram:\n",
    "            H = self.engram(H, x) + H\n",
    "        H = self.attn(H) + H\n",
    "        H = self.ffn(H) + H\n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3064359b-08e6-4d57-9fdd-93c3644fb9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelWithEngram(nn.Module):\n",
    "    def __init__(self, \n",
    "                 config,\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_hc = config.n_hc\n",
    "        self.embd = nn.Embedding(config.vocab_size, config.dim)\n",
    "        self.decoder_block = nn.ModuleList([\n",
    "            DecoderBlock(\n",
    "                 config = config,\n",
    "                 with_engram = (layer_id+1) % 2, # 0, [1], 2, [3]\n",
    "                 layer_id = layer_id,\n",
    "            ) for layer_id in range(config.num_layer)])\n",
    "        self.lm_head = nn.Linear(config.dim, config.vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        H = self.embd(x)\n",
    "        \n",
    "        # hc branch expand\n",
    "        H = H.unsqueeze(2).expand(-1, -1, self.n_hc, -1) # B, L, n_HC, D\n",
    "\n",
    "        for block in self.decoder_block:\n",
    "            H = block(H, x)\n",
    "\n",
    "        # hc branch sum\n",
    "        H = H.sum(dim = 2)\n",
    "\n",
    "        logits = self.lm_head(H)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c5afd9-2d7c-4565-b000-3bc40c6c3d54",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e732787-c8cd-4593-b01a-fdbdd952bd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModelWithEngram(config)\n",
    "x = torch.randint(config.vocab_size, (bsz, seq_len))\n",
    "logits = model(x)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834a5a7f-6edc-48f7-8cd1-f24798305226",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
